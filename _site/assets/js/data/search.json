[
  
  {
    "title": "Model Deployment with KServe and KServe Features",
    "url": "/posts/model-deploymet-kserve/",
    "categories": "",
    "tags": "",
    "date": "2022-11-20 00:32:00 +0100",
    





    
    "snippet": "On a Friday Evening, Mid-September 2019 - Coworkers were leaving the office. The evening was welcoming a bright night. And I was working to deploy the LSTM models on production. I knew - ‚Äúdon‚Äôt dep...",
    "content": "On a Friday Evening, Mid-September 2019 - Coworkers were leaving the office. The evening was welcoming a bright night. And I was working to deploy the LSTM models on production. I knew - ‚Äúdon‚Äôt deploy anything on Friday‚Äù. But I rejected the guidance. I applied a small change on production Friday morning. Unfortunately, the model was not serving any requests due to poor infrastructure. At midnight - I was able to deploy the models perfectly. While returning home - I was sharing the bus with the returned party hoppers. You feel the pain right?Now, It‚Äôs 2022. The little hummingbird has fluttered her wings 7.5 billion times in the last three years. And We have now a simple pluggable solution for many burning machine learning problems like:  Cost: Is the model over or under-scaled? Are resources being used efficiently?  Monitoring: Are all the endpoints healthy? What is the performance profile and request trace?  Rollouts: Is this rollout safe? How do I roll back? Can I test a change without swapping traffic?  Protocol Standards: How do I make a prediction? GRPC? HTTP? Kafka  How do I handle batch predictions?  How do I leverage standardized Data Plane protocol so that I can move the model across MLServing Platforms?  How do I serve Tensorflow, Xgboost, and Pytorch on the same infrastructure?  How do I explain predictions?  How do I wire up custom pre and post-processing?And the Answer is KServe.  KServe is a Model Inferencing Platform on Kubernetes.  Run Anywhere Kubernetes runs. Provides Performant, Standardized inference protocol across ML Frameworks.  Support modern serverless inference workload with Autoscaling including a scale to zero on GPU.  Simple and Pluggable Production Serving including Prediction, pre/post-processing, monitoring, and explainable.  Advanced deployments with Canary rollout, transformers, experiments, ensembles, and Model-Mesh.Starting KServe on EKSWe need a cluster with K8 version 1.22(minimum). We created a cluster called dataplatform-sandbox_ml to execute the exploration of KServe.Local InstallationIf you are exploring on the local machine - you can install Minikube.curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64sudo install minikube-linux-amd64 /usr/local/bin/minikubeminikube start --memory=max --cpus=maxminikube tunnel#open a new terminalKubernetes Installationcurl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\"chmod +x ./kubectlsudo mv ./kubectl /usr/local/bin/kubectlkubectl cluster-infoKServe InstallationExecute the Hack File:curl -s \"https://raw.githubusercontent.com/kserve/kserve/master/hack/quick_install.sh\" | bashExpected Output:\"üòÄ Successfully installed Istio‚Äù\"üòÄ Successfully installed Knative\" \"üòÄ Successfully installed Cert Manager‚Äù\"üòÄ Successfully installed KServe‚ÄùThe Version Matrix            Component      Version                  Knative      1.4.0              Cert Manager Version      1.14.0              Istio      Giovanni Rovelli              Kserve      0.9      Prerequisite: Model Traininginference_address=http://0.0.0.0:8085management_address=http://0.0.0.0:8085metrics_address=http://0.0.0.0:8082grpc_inference_port=7070grpc_management_port=7071enable_metrics_api=truemetrics_format=prometheusnumber_of_netty_threads=4job_queue_size=10enable_envvars_config=trueinstall_py_dep_per_model=falseservice_envelope=bodymodel_store=/mnt/models/model-storemodel_snapshot={\"name\":\"startup.cfg\",\"modelCount\":1,\"models\":{\"SciBERTSeqClassification\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"SciBERTSeqClassification.mar\",\"minWorkers\":1,\"maxWorkers\":2,\"batchSize\":4,\"maxBatchDelay\":1000,\"responseTimeout\":120}}}}Model StorageKServe supports Azure, hdfs, PVC, S3, URI, storageSpec.We explored PVC ( Persistent Volume Claim) and S3 to store the models. The creation of PV and PVC and the Transfer of the model to PV have been covered here.Captum ExplanationsIn order to understand the word importances and attributions when we make an explanation Request, we use Captum Insights for the HuggingFace Transformers pre-trained model.curl --location --request POST 'http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/${MODEL_NAME}/explain' --header 'Host: ${SERVICE_HOSTNAME}' --header 'Content-Type: application/json'--data-raw '{\"id\": \"d3b15cad-50a2-4eaf-80ce-8b0a428bd298\",\"inputs\": [{\"name\": \"4b7c7d4a-51e4-43c8-af61-04639f6ef4bc\",\"shape\": -1,\"datatype\": \"BYTES\",\"data\": \"{\"text\":\"Risk assessment implications.\"}\"}]}'Expected Output{\"id\": \"d3b15cad-50a2-4eaf-80ce-8b0a428bd298\", \"model_name\": \"SciBERTSeqClassification\", \"model_version\": \"1.0\", \"outputs\": [{\"name\": \"explain\", \"shape\": [], \"datatype\": \"BYTES\", \"data\": [{\"words\": [\"[CLS]\", \"Risk\", \"assessment\", \"implications\", \"of\", \"site-specific\", \"oral\", \"relative\", \"bioavailability\", \".\", \"[SEP]\"], \"importances\": [0.0, -0.43571255624310423, -0.11062097534384648, 0.11323803203829622, 0.05438679692935377, -0.11364841625009202, 0.15214504085858935, -0.0013061684457894148, 0.05712844103997178, -0.02296408323390218, 0.1937543236757826, -0.12138265438655091, 0.20713335609474381, -0.8044260616647264, 0.0], \"delta\": -0.019047775223331675}]}]}Inference BatchingKServe supports batch prediction for any ML framework (TensorFlow, PyTorch, ‚Ä¶) without decreasing the performance.This batcher is implemented in the KServe model agent sidecar, so the requests first hit the agent sidecar, when a batch prediction is triggered the request is then sent to the model server container for inference.  maxBatchSize: the max batch size for triggering a prediction.  maxLatency: the max latency for triggering a prediction (In milliseconds).  timeout: timeout of calling predictor service (In seconds).All of the fields have default values in the code. You can config them or not as you wish.apiVersion: \"serving.kserve.io/v1beta1\"kind: \"InferenceService\"metadata:  name: \"scibert\"spec:  predictor:    timeout: 60    batcher:      maxBatchSize: 32      maxLatency: 5000    model:      modelFormat:        name: pytorch      protocolVersion: v2      storageUri: pvc://task-pv-claim/modelsLoggingWe can add Prometheus and Grafana to the cluster to capture the model health. We need to add the prometheus.io/scrape and  prometheus.io/port under annotationsapiVersion: \"serving.kserve.io/v1beta1\"kind: \"InferenceService\"metadata:  name: \"scibert\"  annotations:    prometheus.io/scrape: 'true'    prometheus.io/port: '8082'spec:  predictor:    timeout: 60    batcher:      maxBatchSize: 32      maxLatency: 5000    model:      modelFormat:        name: pytorch      protocolVersion: v2      storageUri: pvc://task-pv-claim/modelshelm repo add prometheus-community https://prometheus-community.github.io/helm-chartshelm install prometheus prometheus-community/prometheuskubectl expose service prometheus-server --type=NodePort --target-port=9090 --name=prometheus-server-npkubectl expose service prometheus-server --type=NodePort --target-port=9090 --name=prometheus-server-nphelm repo add grafana https://grafana.github.io/helm-chartshelm install my-release grafana/grafanakubectl get secret --namespace default my-release-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echoexport POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=my-release\" -o jsonpath=\"{.items[0].metadata.name}\")kubectl --namespace default port-forward $POD_NAME 3000Auto-ScalingKServe supports the implementation of Knative Pod Autoscaler (KPA) and Kubernetes‚Äô Horizontal Pod Autoscaler (HPA).We can configure InferenceService with field containerConcurrency for a hard limit. The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests.We configure InferenceService with annotation autoscaling.knative.dev/target for a soft limit. The soft limit is a targeted limit rather than a strictly enforced bound, particularly if there is a sudden burst of requests, this value can be exceeded.I deployed triton.yaml on EKS cluster to explore the Autoscaling and I used hey to send loads.apiVersion: \"serving.kserve.io/v1beta1\"kind: \"InferenceService\"metadata:  name: triton-gpu  annotations:    prometheus.io/scrape: 'true'    prometheus.io/port: '8082'spec:  predictor:    timeout: 1    batcher:      maxBatchSize: 32      maxLatency: 5000    containers:    - image: {username}/triton-tensorrt-gpu-predictor:latest      name: triton-container      env:      - name: OMP_NUM_THREADS        value: \"1\"      resources:        limits:          nvidia.com/gpu: 1        requests:          nvidia.com/gpu: 1  transformer:    containers:    - image: {username}/triton-tensorrt-transformer:latest      name: kfserving-container      command:      - \"python\"      - \"-m\"      - \"bert_tokenizer\"Sample Json file :{    \"instances\":[        {            \"text\" : \"Risk assessment implications of site-specific oral relative bioavailability.\"        }    ]}Hey Installationgo install github.com/rakyll/hey@latesthey -m POST -z 60s -D ./triton_sample.json -host triton-gpu.default.example.com http://a6907a1015c574abc96a2e47036d54c5-903913666.us-east-2.elb.amazonaws.com/v1/models/transformer_tensorrt_inference:predictExecution Results      Before the ‚ÄúHey‚Äù Execution - Single pod of transformer and predictor is live.        After 2 Seconds - pods are warming up        After 30 Seconds - pods are running        After 70 Seconds - bomberding done. pods are dying        After 90 Seconds - back square 1  "
  },
  
  {
    "title": "How to Select the Right GPU Instance for Your Team on AWS?",
    "url": "/posts/how-to-select-right-gpu-instance-aws/",
    "categories": "",
    "tags": "",
    "date": "2022-11-20 00:32:00 +0100",
    





    
    "snippet": "Imagination is the exaggeration of the data you have in your brain. I want to train a diffusion model to compose a piece of music on a lovely evening in Amsterdam. But I require an AWS GPU instance...",
    "content": "Imagination is the exaggeration of the data you have in your brain. I want to train a diffusion model to compose a piece of music on a lovely evening in Amsterdam. But I require an AWS GPU instance to achieve this. We, the machine learning engineers, are always baffled about the optimal GPU instance on GPU. I completed a short study on this, and the outcome is that:The Decision Tree to Decide GPUIf you are doing HPC (High Performance Job) like Drug Discovery or High Precision Job, then we suggest following the P (historically called Performance-Heavy) Instance Family. Else we recommend following the G (historically called Graphics-Heavy) Instance Family. I am providing the cost chart for the noted GPU instances.P3 and P4 Instances CostG4 and G5 Instances CostThis image is not related to the GPU instance - I got the picture from Dall-E, it‚Äôs not an actual view. The concept was generated with the diffusion model.Always Don‚Äôt Select GPU on the Price GroundPlease don‚Äôt select the GPU always as per the pricing basis. We executed a little experiment- We trained a Scibert Transformer model with 100K data points.The result on a g4dn.2xlarge machine:  The cost is : (0.752 * 311.25) / 3600  = $0.06We executed on a g5.xlarge machine too. The result is on the same configuration :  The cost is : (1.006 * 197.31) / 3600  = $0.05So if we use a g5.xlarge machine then we can save 20% of our budget.And other benefits of a g5 family over a g4 family are:  NVIDIA Ampere Architecture is modern architecture, it supports all the precision formats.  We should follow the Mixed Precision Training in Pytoch based project.There are different types of floating datatypes - FP32, FP16, TF32, BF16source: NVIDIA BlogWe executed the apple-to-apple comparison with fp16 datatype because tf32 and bf16 need the Ampere Architecture which is available under g5 instances family.PS : What is TF32 and BF16?BF16If you have access to a Ampere or newer hardware you can use bf16 for your training and evaluation. While bf16 has a worse precision than fp16, it has a much much bigger dynamic range. Therefore, if in the past you were experiencing overflow issues while training the model, bf16 will prevent this from happening most of the time. Remember that in fp16 the biggest number you can have is `65535` and any number above that will overflow. A bf16 number can be as large as `3.39e+38` (!) which is about the same as fp32 - because both have 8-bits used for the numerical range.TF32The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total.It‚Äôs magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement.When this is done CUDA will automatically switch to using tf32 instead of fp32 where it‚Äôs possible. This, of course, assumes that the used GPU is from the Ampere series.Like all cases with reduced precision this may or may not be satisfactory for your needs, so you have to experiment and see. According to NVIDIA research the majority of machine learning training shouldn‚Äôt be impacted and showed the same perplexity and convergence as the fp32 training.And I found the detailed AWS GPU Instance details for further read            Architecture      NVIDIA GPU      Instance type      Instance name      Number of GPUs      GPU Memory (per GPU)      GPU Interconnect (NVLink / PCIe)      ThermalDesign Power (TDP) from nvidia-smi      Tensor Cores (mixed-precision)      Precision Support      CPU Type      Nitro based                  Ampere      A100      P4      p4d.24xlarge      8      40 GB      NVLink gen 3 (600 GB/s)      400W      Tensor Cores (Gen 3)      FP64, FP32, FP16, INT8, BF16, TF32      Intel Xeon Scalable (Cascade Lake)      Yes              Ampere      A10G      G5      g5.xlarge      1      24 GB      NA (single GPU)      300W      Tensor Cores (Gen 3)      FP64, FP32, FP16, INT8, BF16, TF32      AMD EPYC      Yes              Ampere      A10G      G5      g5.2xlarge      1      24 GB      NA (single GPU)      300W      Tensor Cores (Gen 3)      FP64, FP32, FP16, INT8, BF16, TF32      AMD EPYC      Yes              Ampere      A10G      G5      g5.4xlarge      1      24 GB      NA (single GPU)      300W      Tensor Cores (Gen 3)      FP64, FP32, FP16, INT8, BF16, TF32      AMD EPYC      Yes              Ampere      A10G      G5      g5.8xlarge      1      24 GB      NA (single GPU)      300W      Tensor Cores (Gen 3)      FP64, FP32, FP16, INT8, BF16, TF32      AMD EPYC      Yes              Ampere      A10G      G5      g5.16xlarge      1      24 GB      NA (single GPU)      300W      Tensor Cores (Gen 3)      FP64, FP32, FP16, INT8, BF16, TF32      AMD EPYC      Yes              Ampere      A10G      G5      g5.12xlarge      4      24 GB      PCIe      300W      Tensor Cores (Gen 3)      FP64, FP32, FP16, INT8, BF16, TF32      AMD EPYC      Yes              Ampere      A10G      G5      g5.24xlarge      4      24 GB      PCIe      300W      Tensor Cores (Gen 3)      FP64, FP32, FP16, INT8, BF16, TF32      AMD EPYC      Yes              Ampere      A10G      G5      g5.48xlarge      8      24 GB      PCIe      300W      Tensor Cores (Gen 3)      FP64, FP32, FP16, INT8, BF16, TF32      AMD EPYC      Yes              Turing      T4G      G5      g5g.xlarge      1      16 GB      NA (single GPU)      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      AWS Graviton2      Yes              Turing      T4G      G5      g5g.2xlarge      1      16 GB      NA (single GPU)      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      AWS Graviton2      Yes              Turing      T4G      G5      g5g.4xlarge      1      16 GB      NA (single GPU)      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      AWS Graviton2      Yes              Turing      T4G      G5      g5g.8xlarge      1      16 GB      NA (single GPU)      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      AWS Graviton2      Yes              Turing      T4G      G5      g5g.16xlarge      2      16 GB      PCIe      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      AWS Graviton2      Yes              Turing      T4G      G5      g5g.metal      2      16 GB      PCIe      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      AWS Graviton2      Yes              Turing      T4      G4      g4dn.xlarge      1      16 GB      NA (single GPU)      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      Intel Xeon Scalable (Cascade Lake)      Yes              Turing      T4      G4      g4dn.2xlarge      1      16 GB      NA (single GPU)      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      Intel Xeon Scalable (Cascade Lake)      Yes              Turing      T4      G4      g4dn.4xlarge      1      16 GB      NA (single GPU)      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      Intel Xeon Scalable (Cascade Lake)      Yes              Turing      T4      G4      g4dn.8xlarge      1      16 GB      NA (single GPU)      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      Intel Xeon Scalable (Cascade Lake)      Yes              Turing      T4      G4      g4dn.16xlarge      1      16 GB      NA (single GPU)      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      Intel Xeon Scalable (Cascade Lake)      Yes              Turing      T4      G4      g4dn.12xlarge      4      16 GB      PCIe      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      Intel Xeon Scalable (Cascade Lake)      Yes              Turing      T4      G4      g4dn.metal      8      16 GB      PCIe      70W      Tensor Cores (Gen 2)      FP32, FP16, INT8      Intel Xeon Scalable (Cascade Lake)      Yes              Volta      V100      P3      p3.2xlarge      1      16 GB      NA (single GPU)      300W      Tensor Cores (Gen 1)      FP64, FP32, FP16      Intel Xeon (Broadwell)      No              Volta      V100      P3      p3.8xlarge      4      16 GB      NVLink gen 2 (300 GB/s)      300W      Tensor Cores (Gen 1)      FP64, FP32, FP16      Intel Xeon (Broadwell)      No              Volta      V100      P3      p3.16xlarge      8      16 GB      NVLink gen 2 (300 GB/s)      300W      Tensor Cores (Gen 1)      FP64, FP32, FP16      Intel Xeon (Broadwell)      No              Volta      V100*      P3      p3dn.24xlarge      8      32 GB      NVLink gen 2 (300 GB/s)      300W      Tensor Cores (Gen 1)      FP64, FP32, FP16      Intel Xeon (Skylake)      Yes              Kepler      K80      P2      p2.xlarge      1      12 GB      NA (single GPU)      149W      No      FP64, FP32      Intel Xeon (Broadwell)      No              Kepler      K80      P2      p2.8xlarge      8      12 GB      PCIe      149W      No      FP64, FP32      Intel Xeon (Broadwell)      No              Kepler      K80      P2      p2.16xlarge      16      12 GB      PCIe      149W      No      FP64, FP32      Intel Xeon (Broadwell)      No              Maxwell      M60      G3      g3s.xlarge      1      8 GB      PCIe      150W      No      FP32      Intel Xeon (Broadwell)      No              Maxwell      M60      G3      g3.4xlarge      1      8 GB      PCIe      150W      No      FP32      Intel Xeon (Broadwell)      No              Maxwell      M60      G3      g3.8xlarge      2      8 GB      PCIe      150W      No      FP32      Intel Xeon (Broadwell)      No              Maxwell      M60      G3      g3.16xlarge      4      8 GB      PCIe      150W      No      FP32      Intel Xeon (Broadwell)      No      Reference  Choosing the right GPU for deep learning on AWS  AWS re:Invent 2021 - How to select Amazon EC2 GPU instances for deep learning (sponsored by NVIDIA)"
  }
  
]
